<div align="left">
<h1>
Mengzi3
</h1>
</div>

<p align="center">
    <img src="./assets/mengzi_logo.png" width="200"/>
<p>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Langboat">Hugging Face</a> | ğŸ¤– <a href="https://modelscope.cn/organization/Langboat">ModelScope</a> |  <a href="https://wisemodel.cn/organization/Langboat">Wisemodel</a> ï½œ ğŸ’¬ <a href="https://github.com/Langboat/Mengzi3/blob/main/assets/wechat.png">WeChat</a> | <a href="https://www.langboat.com/document/mengzi/mengzi-gpt/call">API</a> | <a href="https://www.langboat.com/portal/mengzi-gpt">å­Ÿå­GPT</a>
</p>

# æ¨¡å‹ä»‹ç»/Introduction

æœ¬æ¬¡å¼€æºMengzi3 13Bç³»åˆ—æ¨¡å‹ï¼Œæ¨¡å‹çš„åœ°å€å¦‚ä¸‹:

The address of the open source Mengzi3 13B series model is as follows:

|    |                                                                                      Mengzi3-13B-Base                                                                                      | Mengzi3-13B-Chat |
| :-: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------------: |
| 13B | [ğŸ¤—](https://huggingface.co/Langboat/Mengzi3-13B-Base)Â /Â [ğŸ¤–](https://modelscope.cn/organization/Langboat/Mengzi3-13B-Base)Â / [Wisemodel](https://wisemodel.cn/models/Langboat/Mengzi3-13B-Base) |     æ•¬è¯·æœŸå¾…     |

Mengzi3-13Bæ¨¡å‹åŸºäºLlamaæ¶æ„ï¼Œè¯­æ–™ç²¾é€‰è‡ªç½‘é¡µã€ç™¾ç§‘ã€ç¤¾äº¤ã€åª’ä½“ã€æ–°é—»ï¼Œä»¥åŠé«˜è´¨é‡çš„å¼€æºæ•°æ®é›†ã€‚é€šè¿‡åœ¨ä¸‡äº¿tokensä¸Šè¿›è¡Œå¤šè¯­è¨€è¯­æ–™çš„ç»§ç»­è®­ç»ƒï¼Œæ¨¡å‹çš„ä¸­æ–‡èƒ½åŠ›çªå‡ºå¹¶ä¸”å…¼é¡¾å¤šè¯­è¨€èƒ½åŠ›ã€‚

Mengzi3-13B is based on the Llama architecture, and the corpus is selected from web pages, encyclopedias, social networking, media, news, and high-quality open source data sets. By continuing to train multilingual corpus on trillions of tokens, the model has outstanding Chinese capabilities and takes into account multilingual capabilities.

# å¿«é€Ÿå¼€å§‹/Quickstart

é¦–å…ˆè¿›è¡Œç¯å¢ƒé…ç½®ï¼Œå®‰è£…é¡¹ç›®éœ€è¦çš„ä¾èµ–

First configure the environment and install the dependencies required by the project

```bash
pip install -r requirements.txt
```

ç®€å•ä»£ç è°ƒç”¨ï¼š

Simple demo:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Langboat/Mengzi3-13B-Base", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Langboat/Mengzi3-13B-Base", device_map="auto", trust_remote_code=True)
inputs = tokenizer('ä»‹ç»ä¸€ä¸‹å­Ÿå­ï¼š', return_tensors='pt')
if torch.cuda.is_available():
    inputs = inputs.to('cuda')
pred = model.generate(**inputs, max_new_tokens=512, repetition_penalty=1.1, eos_token_id=tokenizer.eos_token_id)
print(tokenizer.decode(pred[0], skip_special_tokens=True))
```

æˆ‘ä»¬å¦å¤–æä¾›ä¸€ä¸ªæ ·ä¾‹ä»£ç ï¼Œå¯ä»¥å¯¹åŸºåº§æ¨¡å‹è¿›è¡Œå•è½®çš„äº¤äº’æ¨ç†ã€‚

We provide this sample code to perform a single round of interactive reasoning on the base model.

```bash
cd examples
python examples/base_streaming_gen.py --model model_path --tokenizer tokenizer_path
```

# æ€§èƒ½è¯„æµ‹/Evaluation

Mengzi3-13B-Baseåœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­ä¸åŒç­‰å‚æ•°é‡å¤§è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œè¯­è¨€èƒ½åŠ›æˆç»©é¢†å…ˆï¼Œæ•°å­¦å’Œç¼–ç¨‹èƒ½åŠ›ä½äºå‰åˆ—ã€‚

Mengzi3-13B-Base leads in language proficiency and is at the forefront in math and programming proficiency compared to the equivalent large language model in various benchmark tests.

|                            |          MMLU          |          CMMLU          |          OCNLI          | GSM8K | HumanEval |
| :------------------------: | :---------------------: | :---------------------: | :---------------------: | :---: | :-------: |
|     Baichuan2-13B-Base     |          0.530          |          0.489          |          0.433          | 0.528 |   0.171   |
|          Qwen-14B          |          0.589          |          0.539          |          0.550          | 0.613 |   0.323   |
|      ChatGLM3-6B-base      |          0.551          |          0.495          |          0.754          | 0.723 |     -     |
|       InternLM2-20B       |          0.610          |          0.538          |          0.650          | 0.761 |   0.488   |
|      Skywork-13B-base      |          0.557          |          0.524          |          0.426          | 0.558 |     -     |
|       LingoWhale-8B       |          0.541          |          0.495          |          0.352          | 0.550 |   0.329   |
|        DeepSeek-7B        |          0.436          |          0.424          |          0.356          | 0.174 |   0.262   |
|   DeepSeek-MoE-16B-base   |          0.423          |          0.388          |          0.342          | 0.188 |   0.268   |
|       MindSource-7B       |          0.498          |          0.425          |          0.528          |   -   |     -     |
| **Mengzi3-13B-Base** | **0.651 (+6.7%)** | **0.588 (+9.1%)** | **0.776 (+2.9%)** | 0.631 |   0.287   |

> ä»¥ä¸Šç»“æœåŸºäº5-shotï¼ŒMMLU/CMMLU/OCNLIç»“æœæ¥è‡ª[FlagEval](https://flageval.baai.ac.cn/)
>
> The above results are based on 5-shotï¼ŒMMLU/CMMLU/OCNLI results from [FlagEval](https://flageval.baai.ac.cn/)

## æ¨¡å‹å¾®è°ƒ/Finetuning

å¾®è°ƒä»£ç åœ¨finetune_demoæ–‡ä»¶å¤¹ä¸‹ã€‚

é¦–å…ˆéœ€è¦å‡†å¤‡jsonlæ ¼å¼çš„å¾®è°ƒæ•°æ®ã€‚å‚è€ƒ finetune_demo/example.jsonlï¼Œæ¯ä¸€è¡Œä¸ºä¸€æ¡jsonæ•°æ®ï¼Œéœ€æ»¡è¶³ä¸‹é¢æ ¼å¼ï¼š

The finetune code in the finetune_demo folder.
Before run the code, first need to prepare the training data in jsonl format. For details, see finetune_demo/example.jsonl. Each line represents one json data in the following format:

```json
{
  "conversation": [
    {
      "role": "human",
      "text": "hello, how are you?"
    },
    {
      "role": "assistant",
      "text": "I am fine."
    },
    ...
  ]
}
```

ç„¶åè¿è¡Œå…¨å‚æ•°å¾®è°ƒçš„è„šæœ¬ã€‚

Then run the supervised finetune script.

```bash
bash finetune.sh
```

# åè®®/License Agreement

Mengzi3-13B-Baseä¾ç…§Apache 2.0åè®®å¼€æºï¼Œå¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼ŒåŒæ—¶æ”¯æŒå…è´¹å•†ç”¨ã€‚å¦‚éœ€ç”³è¯·å•†ä¸šè®¸å¯è¯ï¼Œè¯·[è”ç³»æˆ‘ä»¬](https://www.langboat.com/form?p=3)ï¼Œå…¶ä»–å•†åŠ¡åˆä½œè¯·è”ç³»[bd@langboat.com](mailto:bd@langboat.com)ã€‚

Mengzi3-13B-Base is open source under the Apache 2.0 protocol, fully open for academic research, and free for commercial use. If you need to apply for business license, please [contact us](https://www.langboat.com/en/form?p=3), other business cooperation, please contact [bd@langboat.com](mailto:bd@langboat.com).
